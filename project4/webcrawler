#!/usr/bin/env python3
import sys
#import html
#import html.parse
#import parseurl

from lxml import html

from html.parser import HTMLParser
from socket import socket, AF_INET, SOCK_STREAM
from urllib.parse import urlparse
import gzip
import re
from queue import Queue, Full, Empty
PORT = 80
BUFF = 4096
LOGIN_URL = 'http://fring.ccs.neu.edu/accounts/login/?next=/fakebook/'
CRLF = '\r\n'

####
import pdb
####


class WebCrawler:

    def __init__(self):
        self.cookie = {}
        self.token=''
        self.session=''

    def handle_redirect(self, headers):
        # Handle 301 and 302 code redirects
        if headers.get("Location"):
            return self.GET(headers["Location"])
        return headers, None

    def parse_headers(self, data):
        # Scans data and parses headers. Returns headers in dict format. (Includes status and status code)
        headers = {"Set-Cookie": []}
        newline_counter = 0
        header_regex = re.compile("(.+):(.+)")
        setcookie_regex = re.compile("(\w+)=(\w+);(.+)")
        status_regex = re.compile("(HTTP\/1\.1) +?(\d+)(.+)")
        for line in data.split("\n"):
            if line.strip() == "\n":
                newline_counter += 1
                continue
            # Sometimes a double line break indicates end of header
            if newline_counter == 2:
                break
            # Break at first line of HTML
            if len(line.strip()) > 0 and line.strip()[0] == "<":
                break

            # Parse for status code and message
            if "HTTP/1.1" in line:
                status = status_regex.match(line)
                headers["status"] = status.group(2) + status.group(3)
                headers["statuscode"] = int(status.group(2))

            # Try and find the header key and value
            if header_regex.match(line):
                # Too lazy to find a regex that will accurately extract data so just using substrings to extract header key-value pair
                key = line[:line.index(":")]
                value = line[line.index(":") + 1:]

                if key not in headers:
                    headers[key] = value.strip()
                elif type(headers[key]) == list:
                    headers[key].append(value.strip())
                else:
                    headers[key] = [headers[key], value.strip()]

                # Process Set-Cookie headers

        for header in headers["Set-Cookie"]:
            cookie_extraction = setcookie_regex.match(header)
            self.cookie[cookie_extraction.group(1)] = cookie_extraction.group(2)
            if cookie_extraction.group(1) == "csrftoken":
                self.token = cookie_extraction.group(2)
            if cookie_extraction.group(1) == "sessionid":
                self.session = cookie_extraction.group(2)

        return headers

    def prep_url(self, url):
        p_url = urlparse(url)
        path = '/' if not p_url.path else p_url.path
        if p_url.query:
            path += '?'+p_url.query
        return path, p_url.netloc

    def request(self, host, request):
        # Open socket and send request. Returns decoded response data
        sock = socket(AF_INET, SOCK_STREAM)
        sock.connect((host, PORT))
        sock.send(request.encode('utf-8'))
        return sock.recv(BUFF).rstrip().decode(), sock

    def GET(self,url):
        path, host = self.prep_url(url)

        request = 'GET '+path+' HTTP/1.1\nHost: '+host
        if self.token and self.session:
            request+='\nConnection: keep-alive\n'+\
                       'Cookie: {}'.format(" ".join(["{}={};".format(key, value) for key, value in self.cookie.items()]))+\
                       'Cache-Control: max-age=0\n'+\
                       'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n'+\
                       'User-Agent: ChrisEric/1.1\n'+\
                       'Upgrade-Insecure-Requests: 1\n'+\
                       'Accept-Language: en-US,en;q=0.8\n'#+\
                       #'Accept-Encoding: gzip, deflate, sdch'+\
        request+='\n\r\n'
        data, sock = self.request(host, request)
        sock.settimeout(0.2)
        while True:
            init_length = len(data)
            try:
                data += sock.recv(BUFF).decode()
            except:
                break
            if init_length == len(data):
                break

        sock.close()
        headers = self.parse_headers(data)
        return headers, data

    def POST(self,url, **kwargs):
        p_url = urlparse(url)
        path = '/' if not p_url.path else p_url.path
        host = p_url.netloc

        content = ''
        count = 0
        for key,val in kwargs.items():
            # if count > 0:
            #     content+='&'
            # content += key+'='+val
            # count+=1
            if count > 0:
                content += ';'
            content += "{}={}".format(key, val)
            count += 1

        msg = "POST {} HTTP/1.1\n".format(path)+\
              "Host: {}\n".format(host)+\
              "Connection: keep-alive\n"+\
              "Content-Length: {}\n".format(str(len(content)))+\
              "Content-Type: application/x-www-form-urlencoded\n"+\
              "Cache-Control: max-age=0\n"+\
              "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n"+\
              "Origin: http://{}\n".format(host)+\
              "Upgrade-Insecure-Requests: 1\n"+\
              "User-Agent: ChrisEric/1.1\n"+\
              "Referer: {}\n".format(LOGIN_URL)+\
              "Accept-Language: en-US\n"+\
              "Cookie: {}\n\n".format(" ".join(["{}={};".format(key, value) for key, value in self.cookie.items()]))
        msg+=content+CRLF

        print(msg)
        data, sock = self.request(host, msg)
        sock.close()
        headers = self.parse_headers(data)

        return headers, data

    def fb_login(self,user,pwd):
        headers, data = self.GET(LOGIN_URL)
        postheaders, postdata = self.POST(LOGIN_URL,
                  username=user,
                  password=pwd,
                  next='%2Ffakebook%2F',
                  csrfmiddlewaretoken=self.token)
        print("POST RESPONSE")
        print(postdata)
        self.main_page = self.GET(postheaders['Location']) if 'Location' in postheaders else "No main page"

    def get_initial_ids(self, html):
        return [link.group(2) for link in re.finditer('(href="/fakebook/(\d+?)/")+',html)]

    def crawl(self):
        #pdb.set_trace()
        print("MAIN PAGE: ")
        print(self.main_page)

        pdb.set_trace()
        bfs_queue = Queue()
        initial_ids = self.get_initial_ids(self.main_page[1])
        for id in initial_ids:
            bfs_queue.put(id,block=True)
        pass

if __name__ == '__main__':
    if len(sys.argv) < 3:
        print("username and password required: ./webcrawl <username> <password>")
        sys.exit()

    username = sys.argv[1]
    password = sys.argv[2]

    crawler = WebCrawler()
    crawler.fb_login(username,password)
    crawler.crawl()

    # tree = html.fromstring(data)
    # print(tree)
