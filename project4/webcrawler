#!/usr/bin/env python3
import sys
#import html
#import html.parse
#import parseurl

from lxml import html

from html.parser import HTMLParser
from socket import socket, AF_INET, SOCK_STREAM
from urllib.parse import urlparse
import gzip
PORT = 80
BUFF = 4096
LOGIN_URL = 'http://fring.ccs.neu.edu/accounts/login/?next=/fakebook\n'
CRLF = '\r\n'

####
import pdb
####


class WebCrawler:

    def __init__(self):
        self.cookie=""

    def request(self, host, request):
        # Open socket and send request. Returns decoded response data
        sock = socket(AF_INET, SOCK_STREAM)
        sock.connect((host, PORT))
        sock.send(request.encode('utf-8'))
        return sock.recv(BUFF).rstrip().decode(), sock

    def prep_url(self, url):
        p_url = urlparse(url)
        path = '/' if not p_url.path else p_url.path
        if p_url.query:
            path += '?'+p_url.query
        return path, p_url.netloc

    def GET(self,url):
        path, host = self.prep_url(url)
        request = 'GET {} HTTP/1.1\n'.format(path) + \
        'Host: {}\n'.format(host) + \
        'Connection: keep-alive\n' + \
        'Accept: text/html,application/xhtml+xml\n' + \
        'User-Agent: ChrisEric/1.1\n' + \
        'Referer: http://{}\n'.format(path) + \
        'Accept-Language: en-US\n' + \
        'Cookie:{}\n'.format(self.cookie)

        data, sock = self.request(host, request)
        for line in data.split("\n"):
            print(line)

        while True:
            init_length = len(data)
            data += sock.recv(BUFF).decode()
            if init_length == len(data):
                break

        sock.close()
        return data

    def POST(self,url, **kwargs):
        p_url = urlparse(url)
        path = '/' if not p_url.path else p_url.path
        host = p_url.netloc

        content = ''
        count = 0
        for key,val in kwargs.items():
            if count > 0:
                content+='&'
            content += key+'='+val
            count+=1
        content+='&DEBUG=True'+CRLF

        msg = "POST {} HTTP/1.1".format(path)+CRLF+\
              "Host: {}".format(host)+CRLF+\
              "Connection: keep-alive"+CRLF+\
              "Content-Length: {}".format(str(len(content)))+CRLF+\
              "Content-Type: application/x-www-form-urlencoded"+CRLF+\
              "Cache-Control: max-age=0"+CRLF+\
              "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"+CRLF+\
              "Origin: http://{}".format(host)+CRLF+\
              "Upgrade-Insecure-Requests: 1"+CRLF+\
              "User-Agent: ChrisEric/1.1"+CRLF+\
              "Referer: {}".format(LOGIN_URL)+CRLF+\
              "Accept-Language: en-US"+CRLF+\
              "Cookie: csrftoken={}".format(self.token)+CRLF#+\
              #"X-CSRFToken: {}".format(self.token)+CRLF
              #"Accept-Encoding: gzip, deflate"+CRLF+\ TODO it will send encoded data if we let it
        msg+=content
        print(msg)
        sock = socket(AF_INET, SOCK_STREAM)
        sock.connect((host,PORT))
        sock.send(msg.encode('utf-8'))
        data = sock.recv(BUFF).rstrip().decode()
        print(data)
        sock.close()

    def fb_login(self,user,pwd):
        data = self.GET(LOGIN_URL)
        parse = LoginHtmlParse()
        parse.feed(data)
        parse.close()
        self.token = parse.token
        self.POST(LOGIN_URL,
                  username=user,
                  password=pwd,
                  next='%2Ffakebook%2F',
                  csrfmiddlewaretoken=self.token)

    def crawl(self):
        pass

class LoginHtmlParse(HTMLParser):
    def handle_starttag(self,tag,attrs):
        if tag == 'input':
            for attr in attrs:
                if len(attr) > 1 and attr[1] == 'csrfmiddlewaretoken':
                    for attr2 in attrs:
                        if attr2[0] == 'value':
                            self.token = attr2[1]
                            return

if __name__ == '__main__':
    if len(sys.argv) < 3:
        print("username and password required: ./webcrawl <username> <password>")
        sys.exit()

    username = sys.argv[1]
    password = sys.argv[2]

    crawler = WebCrawler()
    crawler.fb_login(username,password)
    crawler.crawl()

    # tree = html.fromstring(data)
    # print(tree)
